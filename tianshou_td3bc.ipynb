{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:30:53.295303400Z",
     "start_time": "2024-03-14T06:30:47.841350100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from tianshou.data import (\n",
    "    Collector,\n",
    "    CollectStats,\n",
    "    PrioritizedVectorReplayBuffer,\n",
    "    ReplayBuffer,\n",
    "    VectorReplayBuffer,\n",
    ")\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import TD3BCPolicy\n",
    "from tianshou.policy.base import BasePolicy\n",
    "from tianshou.trainer import OfflineTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "from tianshou.utils.space_info import SpaceInfo\n",
    "from tianshou.exploration import GaussianNoise\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from preprocess import preprocess\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gym.register(\n",
    "    id=\"MultiDatasetDiscretedTradingEnv\",\n",
    "    entry_point=\"tianshou_env:MultiDatasetDiscretedTradingEnv\",\n",
    "    disable_env_checker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f95c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg = dict(\n",
    "    id=\"MultiDatasetDiscretedTradingEnv\",\n",
    "    dataset_dir=\"./data/futures/5m/**/**/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    positions=[-1, 0, 1],\n",
    "    multiplier=range(1, 51),\n",
    "    trading_fees=0.0001,\n",
    "    borrow_interest_rate=0.0003,\n",
    "    portfolio_initial_value=1e3,\n",
    "    max_episode_duration=\"max\",\n",
    "    verbose=0,\n",
    "    window_size=60,\n",
    "    btc_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a91dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(**env_cfg)\n",
    "space_info = SpaceInfo.from_env(env)\n",
    "state_shape = space_info.observation_info.obs_shape\n",
    "action_shape = space_info.action_info.action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2408aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"TradingEnv\"\n",
    "expert_data_task = \"trading-env\"\n",
    "\n",
    "seed = 42\n",
    "scale_obs = 0\n",
    "eps_test = 0.005\n",
    "eps_train = 1.0\n",
    "eps_train_final = 0.05\n",
    "replay_buffer_size = 100000\n",
    "\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-4\n",
    "\n",
    "alpha = 0.6\n",
    "beta = 0.4\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "num_fractions = 32\n",
    "num_cosines = 64\n",
    "ent_coef = 10.0\n",
    "hidden_sizes = [512]\n",
    "target_update_freq = 500\n",
    "update_per_step = 0.1\n",
    "\n",
    "exploration_noise = 0.1\n",
    "policy_noise = 0.2\n",
    "update_actor_freq = 2\n",
    "noise_clip = 0.05\n",
    "\n",
    "logdir = \"log\"\n",
    "render = 0.0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "frames_stack = 4\n",
    "resume_path = None\n",
    "resume_id = None\n",
    "logger = \"tensorboard\"\n",
    "wandb_project = \"trading.benchmark\"\n",
    "watch = False\n",
    "save_buffer_name = None\n",
    "\n",
    "reward_threshold = 1e4\n",
    "num_train_envs = 256\n",
    "num_test_envs = 32\n",
    "batch_size = 32\n",
    "n_step = batch_size * num_train_envs\n",
    "\n",
    "epoch = 100\n",
    "step_per_epoch = 8000\n",
    "step_per_collect = num_train_envs * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596b2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = DummyVectorEnv([lambda: gym.make(**env_cfg) for _ in range(num_train_envs)])\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(**env_cfg) for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes[-1],\n",
    "    hidden_sizes=hidden_sizes[:-1],\n",
    "    device=device,\n",
    "    softmax=False,\n",
    ")\n",
    "actor = Actor(\n",
    "    actor_net,\n",
    "    action_shape,\n",
    "    hidden_sizes,\n",
    "    num_cosines=num_cosines,\n",
    "    device=device,\n",
    ")\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_net_1 = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes[-1],\n",
    "    hidden_sizes=hidden_sizes[:-1],\n",
    "    device=device,\n",
    "    softmax=False,\n",
    ")\n",
    "critic_net_2 = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes[-1],\n",
    "    hidden_sizes=hidden_sizes[:-1],\n",
    "    device=device,\n",
    "    softmax=False,\n",
    ")\n",
    "critic_1 = Critic(critic_net_1, device=device)\n",
    "critic_2 = Critic(critic_net_2, device=device)\n",
    "critic_1_optim = torch.optim.Adam(critic_net_1.parameters(), lr=critic_lr)\n",
    "critic_2_optim = torch.optim.Adam(critic_net_2.parameters(), lr=critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb84657",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy: TD3BCPolicy = TD3BCPolicy(\n",
    "    actor=actor,\n",
    "    actor_optim=actor_optim,\n",
    "    critic=critic_1,\n",
    "    critic_optim=critic_1_optim,\n",
    "    critic2=critic_2,\n",
    "    critic2_optim=critic_2_optim,\n",
    "    tau=tau,\n",
    "    gamma=gamma,\n",
    "    exploration_noise=GaussianNoise(sigma=exploration_noise),\n",
    "    policy_noise=policy_noise,\n",
    "    update_actor_freq=update_actor_freq,\n",
    "    noise_clip=noise_clip,\n",
    "    alpha=alpha,\n",
    "    estimation_step=n_step,\n",
    "    action_space=env.action_space,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_path:\n",
    "    policy.load_state_dict(torch.load(resume_path, map_location=device))\n",
    "    print(\"Loaded agent from: \", resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f557769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collector = Collector(policy, test_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92829202",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(logdir, task, \"td3bc\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f974dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_fn(policy: BasePolicy) -> None:\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "def stop_fn(mean_rewards: float) -> bool:\n",
    "    return mean_rewards >= reward_threshold\n",
    "\n",
    "def train_fn(epoch: int, env_step: int) -> None:\n",
    "    # eps annnealing, just a demo\n",
    "    if env_step <= 10000:\n",
    "        policy.set_eps(eps_train)\n",
    "    elif env_step <= 50000:\n",
    "        eps = eps_train - (env_step - 10000) / 40000 * (0.9 * eps_train)\n",
    "        policy.set_eps(eps)\n",
    "    else:\n",
    "        policy.set_eps(0.1 * eps_train)\n",
    "\n",
    "def test_fn(epoch: int, env_step: int | None) -> None:\n",
    "    policy.set_eps(eps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6695ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = OfflineTrainer(\n",
    "    policy=policy,\n",
    "    buffer=replay_buffer,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=epoch,\n",
    "    step_per_epoch=step_per_epoch,\n",
    "    episode_per_test=num_test_envs,\n",
    "    batch_size=batch_size,\n",
    "    save_best_fn=save_best_fn,\n",
    "    logger=logger,\n",
    ").run()\n",
    "assert stop_fn(result.best_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
