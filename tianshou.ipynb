{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T06:30:53.295303400Z",
     "start_time": "2024-03-14T06:30:47.841350100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from tianshou.data import (\n",
    "    Collector,\n",
    "    CollectStats,\n",
    "    PrioritizedVectorReplayBuffer,\n",
    "    ReplayBuffer,\n",
    "    VectorReplayBuffer,\n",
    ")\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import FQFPolicy\n",
    "from tianshou.policy.base import BasePolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils.net.discrete import (\n",
    "    FractionProposalNetwork,\n",
    "    FullQuantileFunction,\n",
    ")\n",
    "from tianshou.utils.space_info import SpaceInfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from preprocess import preprocess\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# set_seed(42)\n",
    "\n",
    "gym.register(\n",
    "    id=\"MultiDatasetDiscretedTradingEnv\",\n",
    "    entry_point=\"tianshou_env:MultiDatasetDiscretedTradingEnv\",\n",
    "    disable_env_checker=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f95c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg = dict(\n",
    "    id=\"MultiDatasetDiscretedTradingEnv\",\n",
    "    dataset_dir=\"./data/futures/5m/**/**/*.pkl\",\n",
    "    preprocess=preprocess,\n",
    "    positions=[-1, 0, 1],\n",
    "    multiplier=range(1, 51),\n",
    "    trading_fees=0.0001,\n",
    "    borrow_interest_rate=0.0003,\n",
    "    portfolio_initial_value=1e3,\n",
    "    max_episode_duration=\"max\",\n",
    "    verbose=0,\n",
    "    window_size=60,\n",
    "    btc_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a91dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(**env_cfg)\n",
    "space_info = SpaceInfo.from_env(env)\n",
    "state_shape = space_info.observation_info.obs_shape\n",
    "action_shape = space_info.action_info.action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2408aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"TradingEnv\"\n",
    "seed = 42\n",
    "scale_obs = 0\n",
    "eps_test = 0.005\n",
    "eps_train = 1.0\n",
    "eps_train_final = 0.05\n",
    "buffer_size = 100000\n",
    "lr = 5e-5\n",
    "fraction_lr = 2.5e-9\n",
    "alpha = 0.6\n",
    "beta = 0.4\n",
    "gamma = 0.99\n",
    "num_fractions = 32\n",
    "num_cosines = 64\n",
    "ent_coef = 10.0\n",
    "hidden_sizes = [512]\n",
    "target_update_freq = 500\n",
    "update_per_step = 0.1\n",
    "logdir = \"log\"\n",
    "render = 0.0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "frames_stack = 4\n",
    "resume_path = None\n",
    "resume_id = None\n",
    "logger = \"tensorboard\"\n",
    "wandb_project = \"trading.benchmark\"\n",
    "watch = False\n",
    "save_buffer_name = None\n",
    "\n",
    "reward_threshold = 1e4\n",
    "num_train_envs = 256\n",
    "num_test_envs = 32\n",
    "batch_size = 32\n",
    "n_step = batch_size * num_train_envs\n",
    "\n",
    "epoch = 100\n",
    "step_per_epoch = 8000\n",
    "step_per_collect = num_train_envs * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596b2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = DummyVectorEnv([lambda: gym.make(**env_cfg) for _ in range(num_train_envs)])\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(**env_cfg) for _ in range(num_test_envs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "439787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_net = Net(\n",
    "    state_shape,\n",
    "    hidden_sizes[-1],\n",
    "    hidden_sizes=hidden_sizes[:-1],\n",
    "    device=device,\n",
    "    softmax=False,\n",
    ")\n",
    "net = FullQuantileFunction(\n",
    "    feature_net,\n",
    "    action_shape,\n",
    "    hidden_sizes,\n",
    "    num_cosines=num_cosines,\n",
    "    device=device,\n",
    ")\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "fraction_net = FractionProposalNetwork(num_fractions, net.input_dim)\n",
    "fraction_optim = torch.optim.RMSprop(fraction_net.parameters(), lr=fraction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb84657",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy: FQFPolicy = FQFPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    fraction_model=fraction_net,\n",
    "    fraction_optim=fraction_optim,\n",
    "    action_space=env.action_space,\n",
    "    discount_factor=gamma,\n",
    "    num_fractions=num_fractions,\n",
    "    ent_coef=ent_coef,\n",
    "    estimation_step=n_step,\n",
    "    target_update_freq=target_update_freq,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "buf: ReplayBuffer\n",
    "prioritized_replay = True\n",
    "\n",
    "if prioritized_replay:\n",
    "    buf = PrioritizedVectorReplayBuffer(\n",
    "        buffer_size,\n",
    "        buffer_num=len(train_envs),\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "    )\n",
    "else:\n",
    "    buf = VectorReplayBuffer(buffer_size, buffer_num=len(train_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f557769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector = Collector(policy, train_envs, buf, exploration_noise=True)\n",
    "test_collector = Collector(policy, test_envs, exploration_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collector.reset()\n",
    "train_collector.collect(n_step=n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92829202",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(logdir, task, \"fqf\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f974dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_fn(policy: BasePolicy) -> None:\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "def stop_fn(mean_rewards: float) -> bool:\n",
    "    return mean_rewards >= reward_threshold\n",
    "\n",
    "def train_fn(epoch: int, env_step: int) -> None:\n",
    "    # eps annnealing, just a demo\n",
    "    if env_step <= 10000:\n",
    "        policy.set_eps(eps_train)\n",
    "    elif env_step <= 50000:\n",
    "        eps = eps_train - (env_step - 10000) / 40000 * (0.9 * eps_train)\n",
    "        policy.set_eps(eps)\n",
    "    else:\n",
    "        policy.set_eps(0.1 * eps_train)\n",
    "\n",
    "def test_fn(epoch: int, env_step: int | None) -> None:\n",
    "    policy.set_eps(eps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6695ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = OffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=epoch,\n",
    "        step_per_epoch=step_per_epoch,\n",
    "        step_per_collect=step_per_collect,\n",
    "        episode_per_test=num_test_envs,\n",
    "        batch_size=batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "        update_per_step=update_per_step,\n",
    "    ).run()\n",
    "assert stop_fn(result.best_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
